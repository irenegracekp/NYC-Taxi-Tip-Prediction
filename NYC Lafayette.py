# -*- coding: utf-8 -*-
"""Copy_of_nyclafyette_ruch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UjI50b87GBWbGwBABv-ujaXr869ymli5
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from scipy import stats
from math import sqrt

master_df = pd.read_csv("/content/drive/MyDrive/NYC/2021_Yellow_Taxi_Trip_Data-2.csv",low_memory=False)
print(master_df.head())

"""# Pre-Processing"""

master_df.head()

#finding outliers in tip amount
master_df['tip_amount'].nlargest(10)

plt.xlim(0,20)
sns.histplot(data=master_df, x="tip_amount",bins =2000)
plt.show()

master_df['tip_amount'].value_counts()

"""**Discovered that a lot of people are tipping almost 0 dollars. Need to find the reason behind this.**"""

for i in range(7):
  print("Payment type " +str(i)+ " customer count: ",len(master_df[(master_df['payment_type']==i)]))

no_tip_count=master_df.groupby('payment_type')['tip_amount'].apply(lambda x: (x==0).sum()).reset_index(name='count')
print(no_tip_count)

"""From above we can see, that 99% of the customers paying using the paymment type 2(cash), 3(no charge), 4(dispute) and 5(unknown) are tipping 0 dollars. The reason behind this is that cash tips are not recorded in the dataset. Other payment type tips are also either not recorded or have faulty data collection. Either ways, we need to drop these rows to reduce the noise. """

master_df.shape

#Dropped rows where payment is not done by credit card and where fare amount is not greater than zero
master_df = master_df[(master_df.payment_type==1) & (master_df.fare_amount >0)]

master_df.shape

#Converted to date time format
master_df['tpep_pickup_datetime'] = pd.to_datetime(master_df['tpep_pickup_datetime'], format = '%m/%d/%Y %I:%M:%S %p', errors = 'raise')
master_df['tpep_dropoff_datetime'] = pd.to_datetime(master_df['tpep_dropoff_datetime'], format = '%m/%d/%Y %I:%M:%S %p', errors = 'raise')

master_df['pickup_weekday']=master_df.tpep_dropoff_datetime.dt.dayofweek

master_df['pickup_weekday'].value_counts()

master_df['pickup_hour']=master_df.tpep_pickup_datetime.dt.hour

master_df['pickup_hour'].value_counts()

master_df.loc[ (master_df['pickup_hour'] >= 0) & (master_df['pickup_hour'] < 6), 'pickup_hour'] = 0    # 0 - Early Morning
master_df.loc[ (master_df['pickup_hour'] >= 6) & (master_df['pickup_hour'] < 12), 'pickup_hour'] = 1    # 1 - Morning
master_df.loc[ (master_df['pickup_hour'] >= 12) & (master_df['pickup_hour'] < 18), 'pickup_hour'] = 2    # 2 - Afternoon
master_df.loc[ (master_df['pickup_hour'] >= 18) & (master_df['pickup_hour'] <= 23), 'pickup_hour'] = 3    # 3 - Night

tip_by_daytime =master_df.groupby('pickup_hour')['tip_amount'].apply(lambda x: x.mean()).reset_index(name='mean')
print(tip_by_daytime)

master_df['dropoff_weekday']=master_df.tpep_dropoff_datetime.dt.dayofweek

master_df['dropoff_weekday'].value_counts()

master_df['drop_hour']=master_df.tpep_dropoff_datetime.dt.hour

master_df['drop_hour'].value_counts()

master_df['RatecodeID'].value_counts()

#Dropped rows where Ratecode ID is 99 beacause rate code ID 99 is not defined
master_df=master_df[master_df.RatecodeID!=99.0]

Location_lookup = pd.read_csv("/content/drive/MyDrive/NYC/taxi+_zone_lookup.csv",low_memory=False)

master_df1 = pd.merge(master_df,Location_lookup,how ='inner',left_on=master_df['PULocationID'],right_on =Location_lookup['LocationID'] )

master_df1.head(10)

master_df1.loc[ master_df1['Borough'] == 'Queens' , 'Pickup_Area'] = 1
master_df1.loc[ master_df1['Borough'] == 'Bronx' , 'Pickup_Area'] = 2
master_df1.loc[ master_df1['Borough'] == 'Brooklyn' , 'Pickup_Area'] = 3
master_df1.loc[ master_df1['Borough'] == 'Staten Island' , 'Pickup_Area'] = 4
master_df1.loc[ master_df1['Borough'] == 'Manhattan' , 'Pickup_Area'] = 5
master_df1.loc[ (master_df1['Zone'] == 'Newark Airport') | (master_df1['Zone']  == 'JFK Airport') | (master_df1['Zone']  == 'LaGuardia Airport') | (master_df1['Borough'] == 'Unknown' ),  'Pickup_Area'] = 6 #Airport Area

master_df1['Pickup_Area'].value_counts()

master_df1['Pickup_Area'].isnull().sum()

master_df1 = master_df1.drop(['key_0','Borough','service_zone','LocationID','Zone'],axis=1)

master_df = pd.merge(master_df1,Location_lookup,how ='inner',left_on=master_df['DOLocationID'],right_on =Location_lookup['LocationID'] )

master_df.loc[ master_df['Borough'] == 'Queens' , 'Dropoff_Area'] = 1
master_df.loc[ master_df['Borough'] == 'Bronx' , 'Dropoff_Area'] = 2
master_df.loc[ master_df['Borough'] == 'Brooklyn' , 'Dropoff_Area'] = 3
master_df.loc[ master_df['Borough'] == 'Staten Island' , 'Dropoff_Area'] = 4
master_df.loc[ master_df['Borough'] == 'Manhattan' , 'Dropoff_Area'] = 5
master_df.loc[ (master_df['Zone'] == 'Newark Airport') | (master_df['Zone']  == 'JFK Airport') | (master_df['Zone']  == 'LaGuardia Airport') | (master_df['Borough'] == 'Unknown' ),  'Dropoff_Area'] = 6 #Airport Area

master_df['Dropoff_Area'].value_counts()

master_df['Dropoff_Area'].isnull().sum()

master_df = master_df.drop(['key_0','Borough','service_zone','LocationID','Zone','PULocationID','DOLocationID','VendorID','payment_type'],axis=1)

master_df.head(10)

# master_df['tip_fraction'] =  master_df.tip_amount/master_df.fare_amount

master_df['z_score'] = (master_df['tip_amount'] - master_df['tip_amount'].mean()) / master_df['tip_amount'].std(ddof=0)

#remove outliers beyond the third standard deviation point
master_df = master_df[(np.abs(stats.zscore(master_df['z_score'])) < 3)]

master_df['tip_amount'].describe()

plt.figure(figsize=(17,8))
sns.heatmap(master_df.corr('spearman'), annot = True)

master_df['tip_amount'].describe()

X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(
        master_df[['trip_distance', 'RatecodeID','total_amount','tolls_amount','extra','pickup_hour','pickup_weekday','Pickup_Area','Dropoff_Area']], 
        master_df['tip_amount'], 
        test_size=0.3,
        train_size=0.7,        
        random_state=42
    )

"""# Linear Regression"""

model = LinearRegression()
model.fit(X_train_base, y_train_base)
preds = model.predict(X_test_base)

print("R-squared value: ", r2_score(y_test_base,preds))
print("RMSE of Linear Regression: ", (mean_squared_error(y_test_base,preds))**(1/2) )

"""# Decision Tree Regressor"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

depths = [1,5,10,15,20,35,50]
tree_MSE = []
for i in depths:
  tree_model = DecisionTreeRegressor(max_depth = i)
  tree_model.fit(X_train_base, y_train_base)
  tree_test_mse = (mean_squared_error(y_test_base, tree_model.predict(X_test_base)))**(1/2)
  tree_MSE.append(tree_test_mse)
print(tree_MSE)

tree_model = DecisionTreeRegressor(max_depth = 20)
tree_model.fit(X_train_base, y_train_base)

tree_test_mse = (mean_squared_error(y_test_base, tree_model.predict(X_test_base)))**(1/2)
print("RMSE of Decision Tree Regressor: ",tree_test_mse)

"""# Random Forest Regressor"""

random_grid = {'max_depth': [5,10,15,20,25, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [20,60,100,150,200]}

from sklearn.model_selection import RandomizedSearchCV
rf = RandomForestRegressor()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, cv =2, n_iter = 50, verbose=2, random_state=42, n_jobs = -1)

rf_random.fit(X_train_base, y_train_base)

rf_random.best_params_

rf_random.best_score_

rf_model = RandomForestRegressor(n_estimators= 150, min_samples_split= 10, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 15, n_jobs = -1)
rf_model.fit(X_train_base, y_train_base)

rf_test_mse = (mean_squared_error(y_test_base, rf_model.predict(X_test_base)))**(1/2)
print("Feature Importance:")
print(rf_model.feature_importances_)
print("RMSE of Random Forest Regressor: ",rf_test_mse)

"""# K Neighbours Regressor

**SCALING OF DATA**
"""

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X_train = sc_X.fit_transform(X_train_base)
X_test=sc_X.fit_transform(X_test_base)

from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor()

model.fit(X_train, y_train_base)

rmse = (mean_squared_error(y_test_base, model.predict(X_test)))**(1/2)
print("RMSE of K Neighbours Regressor: ",rmse)

"""# XGBoost"""

from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold

XGmodel = XGBRegressor()

XGmodel.fit(X_train_base, y_train_base)
XG_test_mse = (mean_squared_error(y_test_base, XGmodel.predict(X_test_base)))**(1/2)
print(XG_test_mse)

random_grid_XG = {'max_depth': [2,5,8, None],
 'learning_rate': [0.5, 0.3, 0.1, 0.01],
 'subsample': [0.25,0.5,0.75],
 'n_estimators': [20,60,100,150,200]}

XG_random = RandomizedSearchCV(estimator = XGmodel, param_distributions = random_grid_XG, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)

XG_random.fit(X_train_base, y_train_base)

XG_random.best_params_

XG_random.best_score_

XGmodel_final = XGBRegressor(subsample=  0.75, n_estimators= 200, max_depth =  8, learning_rate = 0.3)
XGmodel_final.fit(X_train_base, y_train_base)

XG_test_mse = (mean_squared_error(y_test_base, XGmodel_final.predict(X_test_base)))**(1/2)
print("RMSE of XGBoost Regressor: ",XG_test_mse)

"""# Neural Nets"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline

def baseline_model():
 # create model
 model = Sequential()
 model.add(Dense(9, input_shape=(9,), kernel_initializer='normal', activation='relu'))
 model.add(Dense(1, kernel_initializer='normal'))
 # Compile model
 model.compile(loss='mean_squared_error', optimizer='adam')
 return model

from sklearn.pipeline import Pipeline
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(baseline_model, epochs=100, batch_size= 80000 , verbose=0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits=10)
results = cross_val_score(pipeline, X_train_base, y_train_base, cv=kfold, scoring='neg_root_mean_squared_error')

print(results)
print(results.mean()) #ignore the negative sign as we have considered the negative root mean squared error for scoring purposes.

def larger_model():
 # create model
 model = Sequential()
 model.add(Dense(20, input_shape=(9,), kernel_initializer='normal', activation='relu'))
 model.add(Dense(15, kernel_initializer='normal', activation='relu'))
 model.add(Dense(9, kernel_initializer='normal', activation='relu'))
 model.add(Dense(6, kernel_initializer='normal', activation='relu'))
 model.add(Dense(1, kernel_initializer='normal'))
 # Compile model
 model.compile(loss='mean_squared_error', optimizer='adam')
 print(model.summary())
 return model

from sklearn.pipeline import Pipeline
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(larger_model, epochs=100, batch_size= 80000 , verbose=0)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits=10)
results = cross_val_score(pipeline, X_train_base, y_train_base, cv=kfold, scoring='neg_root_mean_squared_error')

print(results)
print("RMSE of Neural Networks: ",abs(results.mean()))